{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - Authentification validée\n",
      "I - Configuration:  big_heavy_d\n",
      "I - En attente de l'initialisation de l'application Spark ....\n",
      "I - Application Spark lancée !\n"
     ]
    }
   ],
   "source": [
    "# Context creation\n",
    "from dtm_tools.init_spark import CreateContext\n",
    "\n",
    "context = CreateContext(app_name='dad_lot2_1',\n",
    "                        conf='big_heavy_d')       # conf est optionnel, defaut : small_light_d\n",
    "spark = context.get_spark()\n",
    "hive   = context.get_hive(spark)\n",
    "\n",
    "import pandas as pd \n",
    "import os\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    \"Union des tables\"\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "def select_between_dates(df, date1, date2, date_type):\n",
    "\n",
    "    \"Récupère un df et sélectionne les lignes émis entre deux strings dates passées en paramètres selon la colonne date passée en parametre\"\n",
    "    date1 = datetime.strptime(date1, '%d/%m/%Y')\n",
    "    date2 = datetime.strptime(date2, '%d/%m/%Y')\n",
    "    df_out = (df.filter((df[date_type] >= date1) & (df[date_type] <= date2)))\n",
    "    n_rows_deleted = df.count() - df_out.count()\n",
    "    print(\"Nombre de lignes en période d'analyse: \" + str(df_out.count()))\n",
    "    return(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltac = spark.sql(\"select * from db_deltac.deltac_extrn\")\n",
    "deltad = spark.sql(\"select * from db_deltad.deltad_extrn\")\n",
    "deltax = spark.sql(\"select * from db_deltax.deltax_extrn\")\n",
    "\n",
    "deltag = unionAll(deltac,deltad,deltax)\n",
    "\n",
    "#Features casting and creation\n",
    "##Conversion en int des masses et valeur\n",
    "deltag = deltag.withColumn(\"qt_masse_nette\", deltag[\"qt_masse_nette\"].cast(T.IntegerType()))\n",
    "deltag = deltag.withColumn(\"mt_valeur_douane\", deltag[\"mt_valeur_douane\"].cast(T.IntegerType()))\n",
    "##Creation d'une colonne nc4 \n",
    "deltag = deltag.withColumn('nc6',deltag.cd_marchandise.substr(1, 6))\n",
    "deltag = deltag.withColumn('nc4',deltag.cd_marchandise.substr(1, 4))\n",
    "\n",
    "##Creation d'un numéro DAUs sans préfixe\n",
    "deltag = deltag.withColumn(\"dau_number\",F.col(\"id\").substr(6,14))\n",
    "\n",
    "## Creation Id article\n",
    "deltag = deltag.withColumn(\"id_article\",F.concat(F.col(\"dau_number\"),\n",
    "                                                         F.lit(\"_\"),\n",
    "                                                         F.col(\"cd_numero_article\")))\n",
    "#Filter Import \n",
    "deltag = deltag.filter(F.col(\"cd_type_flux\").rlike(\"IMP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes en période d'analyse: 127838204\n"
     ]
    }
   ],
   "source": [
    "#Selection du périmètre par dates d'emission \n",
    "from datetime import datetime\n",
    "deltag_sub = select_between_dates(deltag, date1 = \"01/02/2017\", date2 = \"31/12/2021\", date_type = \"dt_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127838204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltag_sub = deltag_sub.cache()\n",
    "deltag_sub.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decs = deltag_sub.groupBy(\"lb_pays_origine\",\"cd_marchandise\").agg(\n",
    "\n",
    "    F.count(\"id_article\").alias(\"n_articles\"),\n",
    "    F.sum(\"mt_valeur_douane\").alias(\"mt_valeur_douane\"),\n",
    "    F.sum(\"qt_masse_nette\").alias(\"qt_masse_nette\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decs_pd = decs.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.558510e+05\n",
      "mean     4.996588e+02\n",
      "std      5.607397e+04\n",
      "min      1.000000e+00\n",
      "10%      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      5.000000e+00\n",
      "75%      3.100000e+01\n",
      "90%      2.150000e+02\n",
      "max      2.817988e+07\n",
      "Name: n_articles, dtype: float64\n",
      "count    2.547950e+05\n",
      "mean     3.837792e+06\n",
      "std      1.041842e+08\n",
      "min      0.000000e+00\n",
      "10%      1.580000e+02\n",
      "25%      1.187000e+03\n",
      "30%      2.042000e+03\n",
      "40%      5.554000e+03\n",
      "50%      1.425900e+04\n",
      "75%      1.691775e+05\n",
      "90%      1.577151e+06\n",
      "max      2.033298e+10\n",
      "Name: mt_valeur_douane, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(decs_pd[\"n_articles\"].describe([0.1,0.25,0.5,0.75,0.9]))\n",
    "print(decs_pd[\"mt_valeur_douane\"].describe([0.1,0.25,0.3,0.4,0.5,0.75,0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_decs_pd = decs_pd[(decs_pd[\"mt_valeur_douane\"] > 1000000) & (decs_pd[\"n_articles\"] > 100)]\n",
    "sub_decs_pd.columns = ['lb_pays_origine_1', 'cd_marchandise_1', 'n_articles_1', 'mt_valeur_douane_1','qt_masse_nette_1']\n",
    "sub_decs_pd = sub_decs_pd[['lb_pays_origine_1', 'cd_marchandise_1']]\n",
    "selected_ncs = spark.createDataFrame(sub_decs_pd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltag_sub_merged = deltag_sub.join(F.broadcast(selected_ncs),\n",
    "                                    (deltag_sub.cd_marchandise == selected_ncs.cd_marchandise_1) &\n",
    "                                    (deltag_sub.lb_pays_origine == selected_ncs.lb_pays_origine_1), how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64704959"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltag_sub_merged = deltag_sub_merged.cache()\n",
    "deltag_sub_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decs_lot2 = deltag_sub_merged.select(\"dt_validation\",\n",
    "                \"lb_pays_provenance\",\n",
    "                \"lb_pays_origine\",\n",
    "                \"mt_valeur_douane\",\n",
    "                \"cd_marchandise\",\n",
    "                \"cd_operateur_importateur\",\n",
    "                \"lb_operateur_importateur_nom\",\n",
    "                \"id\",\n",
    "                \"qt_masse_nette\",\n",
    "                \"id_article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decs_lot2 = decs_lot2.withColumn(\"ratio_mv\", F.col(\"mt_valeur_douane\") / F.col(\"qt_masse_nette\"))\n",
    "decs_lot2 = decs_lot2.withColumn(\"date\", F.regexp_replace(F.col('dt_validation'),\"T\",\" \"))\n",
    "decs_lot2 = decs_lot2.withColumn(\"date\", F.substring(F.col(\"date\"),1,22))\n",
    "decs_lot2 = decs_lot2.withColumn(\"date\", F.to_timestamp(F.col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decs_lot2_grp = decs_lot2.groupBy(\"lb_pays_origine\",\"cd_marchandise\", F.window(F.col(\"date\"),\"1 week\")).agg(\n",
    "\n",
    "#     F.collect_list(\"id_article\").alias(\"id_articles\"),\n",
    "#     F.collect_list(\"dt_validation\").alias(\"dt_validation\"),\n",
    "#     F.collect_list(\"mt_valeur_douane\").alias(\"mt_valeur_douane\"),\n",
    "#     F.collect_list(\"qt_masse_nette\").alias(\"qt_masse_nette\"),\n",
    "#     F.collect_list(\"ratio_mv\").alias(\"ratio_mv\")\n",
    "    F.sum(\"mt_valeur_douane\").alias(\"mt_valeur_douane\"),\n",
    "    F.sum(\"qt_masse_nette\").alias(\"qt_masse_nette\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2717511\n"
     ]
    }
   ],
   "source": [
    "decs_lot2_grp = decs_lot2_grp.cache()\n",
    "print(decs_lot2_grp.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717511"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_item(liste):\n",
    "    return(liste[0])\n",
    "#Get firstdate\n",
    "first_elem_udf = F.udf(lambda x: get_item(x), T.TimestampType())\n",
    "decs_lot2_grp = decs_lot2_grp.withColumn(\"dates\", first_elem_udf(F.col(\"window\")))\n",
    "decs_lot2_grp = decs_lot2_grp.withColumn(\"dates\", F.date_format(F.col(\"dates\"),\"yyyy-MM-dd\"))\n",
    "decs_lot2_grp = decs_lot2_grp.cache()\n",
    "decs_lot2_grp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def get_full_period_data(startdate,\n",
    "                         endate,\n",
    "                         resampling_parameter):\n",
    "    \"\"\"\n",
    "    Creating a spark dataframe with one column filled with 0 for each dates between, Start and enddate.\n",
    "    Parms : Startdate (string), Endate (string), Resampling parameter\n",
    "    \"\"\"\n",
    "    #Setting-up analysis period dataframe : 2 cols dates and sum filled by zeros\n",
    "    startdate = datetime.strptime(startdate, '%Y-%m-%d')\n",
    "    endate = datetime.strptime(endate, '%Y-%m-%d')\n",
    "    analysis_period = pd.date_range(startdate,endate,freq=resampling_parameter)\n",
    "    analysis_period = pd.DataFrame(analysis_period)\n",
    "    analysis_period.columns = [\"dates\"]\n",
    "    analysis_period[\"mt_valeur_douane\"] = 0\n",
    "    analysis_period[\"qt_masse_nette\"] = 0\n",
    "    analysis_period[\"dates\"] = analysis_period[\"dates\"].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    analysis_period_sq = spark.createDataFrame(analysis_period)\n",
    "    return(analysis_period_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5300886"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a 4 years dataframe with values filled with 0 \n",
    "full_period = get_full_period_data(\"2017-02-01\",\"2021-12-31\",\"W-THU\")\n",
    "full_period = full_period.repartition(10)\n",
    "#Crossjoin in order to product a table with by weeks x cd_marchandise x pays origine filled of zeros\n",
    "##Selecting list of NCs x Lb pays origine\n",
    "ncs_orgin = decs_lot2_grp.select(\"cd_marchandise\",\"lb_pays_origine\").distinct()\n",
    "ncs_orgin = ncs_orgin.cache()\n",
    "ncs_orgin.count()\n",
    "df_0_filled = full_period.crossJoin(F.broadcast(ncs_orgin))\n",
    "##Changing colnames for future join \n",
    "for c in df_0_filled.columns:\n",
    "    df_0_filled = df_0_filled.withColumnRenamed(c,c+\"_0\")\n",
    "##Caching\n",
    "df_0_filled = df_0_filled.cache()\n",
    "df_0_filled.count()  \n",
    "decs_lot2_grp = decs_lot2_grp.repartition(200)\n",
    "\n",
    "#Fulljoin with decs\n",
    "full_join = df_0_filled.join(decs_lot2_grp, ((df_0_filled.dates_0 == decs_lot2_grp.dates) & \n",
    "                                            (df_0_filled.lb_pays_origine_0 == decs_lot2_grp.lb_pays_origine) & \n",
    "                                             (df_0_filled.cd_marchandise_0 == decs_lot2_grp.cd_marchandise)), how = \"full_outer\")\n",
    "\n",
    "full_join = full_join.cache()\n",
    "full_join.count()\n",
    "\n",
    "##Selecting rows wich have no data \n",
    "left_only = full_join.filter(F.col(\"lb_pays_origine\").isNull())\n",
    "##Renaming cols\n",
    "selected_cols = ['dates_0',\n",
    " 'mt_valeur_douane_0',\n",
    " 'qt_masse_nette_0',\n",
    " 'cd_marchandise_0',\n",
    " 'lb_pays_origine_0']\n",
    "left_only = left_only.select(selected_cols)\n",
    "\n",
    "for c in left_only.columns: \n",
    "    left_only = left_only.withColumnRenamed(c, c[:-2])\n",
    "\n",
    "#Dropping windows\n",
    "decs_lot2_grp = decs_lot2_grp.drop(\"window\")\n",
    "\n",
    "##Appending rows with 0 \n",
    "left_only = left_only.select(decs_lot2_grp.columns)\n",
    "augement_decs_lot_2_grp = decs_lot2_grp.union(left_only)\n",
    "\n",
    "#Final data \n",
    "augement_decs_lot_2_grp = augement_decs_lot_2_grp.cache()\n",
    "augement_decs_lot_2_grp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "augement_decs_lot_2_grp.write.parquet(\"lot_2_dad.parquet\")\n",
    "!hadoop fs -copyToLocal /user/nnougue/lot_2_dad.parquet /home/nnougue/poc_monitoring_dad/data/outputs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "234284c5f671860a4954f702437de25749c25f56daa6a04503498d5a3cb82dcf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('py37_poc_sarc': conda)",
   "language": "python",
   "name": "python377jvsc74a57bd04c03ccb3a8d20ba1b7596f3a1a9aedfd0a61957e9f65fd1f54360ff57cd3e98e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
