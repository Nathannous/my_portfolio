{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Import and refactoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import : data 1 row = 1 value - ESSAI 1 : données toutes NCS DAD\n",
    "# df = pd.read_parquet(\n",
    "#     \"../data/inputs/dad_data_filled.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# Import : data 1 row = 1 value - ESSAI 2 : NC sup > 100k€ d'import \n",
    "df = pd.read_parquet(\n",
    "    \"../data/inputs/lot_2_dad.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "# Sorting df by dates\n",
    "df = df.sort_values(\"dates\")\n",
    "# Grouping : 1 array per country x NC x Dates\n",
    "vd_douane_grouped = df.groupby([\"lb_pays_origine\", \"cd_marchandise\"])[\n",
    "    \"mt_valeur_douane\"].apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting index and ffill\n",
    "vd_douane_grouped = vd_douane_grouped.reset_index()\n",
    "vd_douane_grouped[\"lb_pays_origine\"] = vd_douane_grouped[\"lb_pays_origine\"].ffill()\n",
    "# Creating new colname -> orgine x cd_marchandise\n",
    "vd_douane_grouped[\"id\"] = vd_douane_grouped[\"lb_pays_origine\"] + \\\n",
    "    vd_douane_grouped[\"cd_marchandise\"]\n",
    "\n",
    "# Transposing : now : columns = origine x cd_marchandise rows = timesteps\n",
    "dataframe = vd_douane_grouped.T\n",
    "dataframe.columns = vd_douane_grouped[\"id\"]\n",
    "\n",
    "# Keeping only valeur_douane as key feature\n",
    "series = dataframe.loc[\"mt_valeur_douane\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting index and ffill\n",
    "vd_douane_grouped = vd_douane_grouped.reset_index()\n",
    "vd_douane_grouped[\"lb_pays_origine\"] = vd_douane_grouped[\"lb_pays_origine\"].ffill()\n",
    "# Creating new colname -> orgine x cd_marchandise\n",
    "vd_douane_grouped[\"id\"] = vd_douane_grouped[\"lb_pays_origine\"] + \\\n",
    "    vd_douane_grouped[\"cd_marchandise\"]\n",
    "\n",
    "# Transposing : now : columns = origine x cd_marchandise rows = timesteps\n",
    "dataframe = vd_douane_grouped.T\n",
    "dataframe.columns = vd_douane_grouped[\"id\"]\n",
    "\n",
    "# Keeping only valeur_douane as key feature\n",
    "series = dataframe.loc[\"mt_valeur_douane\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(\"level_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21893"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>Afrique du Sud0302541100</th>\n",
       "      <th>Afrique du Sud0304449090</th>\n",
       "      <th>Afrique du Sud0304741100</th>\n",
       "      <th>Afrique du Sud0306310090</th>\n",
       "      <th>Afrique du Sud0603140000</th>\n",
       "      <th>Afrique du Sud0709999090</th>\n",
       "      <th>Afrique du Sud0804400010</th>\n",
       "      <th>Afrique du Sud0805102290</th>\n",
       "      <th>Afrique du Sud0805102810</th>\n",
       "      <th>Afrique du Sud0805102890</th>\n",
       "      <th>...</th>\n",
       "      <th>Viêt-nam9619008100</th>\n",
       "      <th>Viêt-nam9905000000</th>\n",
       "      <th>Zambie0810902090</th>\n",
       "      <th>Zambie2401103510</th>\n",
       "      <th>Zambie7103910000</th>\n",
       "      <th>Zimbabwe0708100000</th>\n",
       "      <th>Zimbabwe2401208590</th>\n",
       "      <th>Zimbabwe2401300000</th>\n",
       "      <th>Zimbabwe4113300000</th>\n",
       "      <th>Zimbabwe6802931000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>21883</td>\n",
       "      <td>21884</td>\n",
       "      <td>21885</td>\n",
       "      <td>21886</td>\n",
       "      <td>21887</td>\n",
       "      <td>21888</td>\n",
       "      <td>21889</td>\n",
       "      <td>21890</td>\n",
       "      <td>21891</td>\n",
       "      <td>21892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lb_pays_origine</th>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>Afrique du Sud</td>\n",
       "      <td>...</td>\n",
       "      <td>Viêt-nam</td>\n",
       "      <td>Viêt-nam</td>\n",
       "      <td>Zambie</td>\n",
       "      <td>Zambie</td>\n",
       "      <td>Zambie</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cd_marchandise</th>\n",
       "      <td>0302541100</td>\n",
       "      <td>0304449090</td>\n",
       "      <td>0304741100</td>\n",
       "      <td>0306310090</td>\n",
       "      <td>0603140000</td>\n",
       "      <td>0709999090</td>\n",
       "      <td>0804400010</td>\n",
       "      <td>0805102290</td>\n",
       "      <td>0805102810</td>\n",
       "      <td>0805102890</td>\n",
       "      <td>...</td>\n",
       "      <td>9619008100</td>\n",
       "      <td>9905000000</td>\n",
       "      <td>0810902090</td>\n",
       "      <td>2401103510</td>\n",
       "      <td>7103910000</td>\n",
       "      <td>0708100000</td>\n",
       "      <td>2401208590</td>\n",
       "      <td>2401300000</td>\n",
       "      <td>4113300000</td>\n",
       "      <td>6802931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mt_valeur_douane</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>Afrique du Sud0302541100</td>\n",
       "      <td>Afrique du Sud0304449090</td>\n",
       "      <td>Afrique du Sud0304741100</td>\n",
       "      <td>Afrique du Sud0306310090</td>\n",
       "      <td>Afrique du Sud0603140000</td>\n",
       "      <td>Afrique du Sud0709999090</td>\n",
       "      <td>Afrique du Sud0804400010</td>\n",
       "      <td>Afrique du Sud0805102290</td>\n",
       "      <td>Afrique du Sud0805102810</td>\n",
       "      <td>Afrique du Sud0805102890</td>\n",
       "      <td>...</td>\n",
       "      <td>Viêt-nam9619008100</td>\n",
       "      <td>Viêt-nam9905000000</td>\n",
       "      <td>Zambie0810902090</td>\n",
       "      <td>Zambie2401103510</td>\n",
       "      <td>Zambie7103910000</td>\n",
       "      <td>Zimbabwe0708100000</td>\n",
       "      <td>Zimbabwe2401208590</td>\n",
       "      <td>Zimbabwe2401300000</td>\n",
       "      <td>Zimbabwe4113300000</td>\n",
       "      <td>Zimbabwe6802931000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21893 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id                                         Afrique du Sud0302541100  \\\n",
       "index                                                             0   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0302541100   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0302541100   \n",
       "\n",
       "id                                         Afrique du Sud0304449090  \\\n",
       "index                                                             1   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0304449090   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0304449090   \n",
       "\n",
       "id                                         Afrique du Sud0304741100  \\\n",
       "index                                                             2   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0304741100   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0304741100   \n",
       "\n",
       "id                                         Afrique du Sud0306310090  \\\n",
       "index                                                             3   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0306310090   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0306310090   \n",
       "\n",
       "id                                         Afrique du Sud0603140000  \\\n",
       "index                                                             4   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0603140000   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0603140000   \n",
       "\n",
       "id                                         Afrique du Sud0709999090  \\\n",
       "index                                                             5   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0709999090   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0709999090   \n",
       "\n",
       "id                                         Afrique du Sud0804400010  \\\n",
       "index                                                             6   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0804400010   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0804400010   \n",
       "\n",
       "id                                         Afrique du Sud0805102290  \\\n",
       "index                                                             7   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0805102290   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0805102290   \n",
       "\n",
       "id                                         Afrique du Sud0805102810  \\\n",
       "index                                                             8   \n",
       "lb_pays_origine                                      Afrique du Sud   \n",
       "cd_marchandise                                           0805102810   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                         Afrique du Sud0805102810   \n",
       "\n",
       "id                                         Afrique du Sud0805102890  ...  \\\n",
       "index                                                             9  ...   \n",
       "lb_pays_origine                                      Afrique du Sud  ...   \n",
       "cd_marchandise                                           0805102890  ...   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
       "id                                         Afrique du Sud0805102890  ...   \n",
       "\n",
       "id                                               Viêt-nam9619008100  \\\n",
       "index                                                         21883   \n",
       "lb_pays_origine                                            Viêt-nam   \n",
       "cd_marchandise                                           9619008100   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                               Viêt-nam9619008100   \n",
       "\n",
       "id                                               Viêt-nam9905000000  \\\n",
       "index                                                         21884   \n",
       "lb_pays_origine                                            Viêt-nam   \n",
       "cd_marchandise                                           9905000000   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                               Viêt-nam9905000000   \n",
       "\n",
       "id                                                 Zambie0810902090  \\\n",
       "index                                                         21885   \n",
       "lb_pays_origine                                              Zambie   \n",
       "cd_marchandise                                           0810902090   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                                 Zambie0810902090   \n",
       "\n",
       "id                                                 Zambie2401103510  \\\n",
       "index                                                         21886   \n",
       "lb_pays_origine                                              Zambie   \n",
       "cd_marchandise                                           2401103510   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                                 Zambie2401103510   \n",
       "\n",
       "id                                                 Zambie7103910000  \\\n",
       "index                                                         21887   \n",
       "lb_pays_origine                                              Zambie   \n",
       "cd_marchandise                                           7103910000   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                                 Zambie7103910000   \n",
       "\n",
       "id                                               Zimbabwe0708100000  \\\n",
       "index                                                         21888   \n",
       "lb_pays_origine                                            Zimbabwe   \n",
       "cd_marchandise                                           0708100000   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                               Zimbabwe0708100000   \n",
       "\n",
       "id                                               Zimbabwe2401208590  \\\n",
       "index                                                         21889   \n",
       "lb_pays_origine                                            Zimbabwe   \n",
       "cd_marchandise                                           2401208590   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                               Zimbabwe2401208590   \n",
       "\n",
       "id                                               Zimbabwe2401300000  \\\n",
       "index                                                         21890   \n",
       "lb_pays_origine                                            Zimbabwe   \n",
       "cd_marchandise                                           2401300000   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                               Zimbabwe2401300000   \n",
       "\n",
       "id                                               Zimbabwe4113300000  \\\n",
       "index                                                         21891   \n",
       "lb_pays_origine                                            Zimbabwe   \n",
       "cd_marchandise                                           4113300000   \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "id                                               Zimbabwe4113300000   \n",
       "\n",
       "id                                               Zimbabwe6802931000  \n",
       "index                                                         21892  \n",
       "lb_pays_origine                                            Zimbabwe  \n",
       "cd_marchandise                                           6802931000  \n",
       "mt_valeur_douane  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "id                                               Zimbabwe6802931000  \n",
       "\n",
       "[5 rows x 21893 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending to final df\n",
    "final = pd.DataFrame()\n",
    "for i in range(len(series)):\n",
    "    try: \n",
    "        final[series.index[i]] = series[i]\n",
    "    except: \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving df\n",
    "final.to_csv(\"../data/inputs/tensor_as_df_211801.csv\", sep=\";\", decimal=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard preprocessing for time serie prediction : \n",
    "\n",
    "- reading data, \n",
    "- scaling, \n",
    "- window definition, \n",
    "- batch definition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Preprocessing\n",
      "-----Train/test split\n",
      "Final shape (257, 21876)\n",
      "Train df shape:  (209, 21876)\n",
      "Test df shape:  (48, 21876)\n",
      "----Scaling\n",
      "(209, 21876)\n",
      "(48, 21876)\n",
      "-----Window transformation\n",
      "n-weeks loopback:  10\n",
      "n-weeks prediction:  2\n",
      "X_train:  (198, 10, 21876)\n",
      "Y_train:  (198, 2, 21876)\n",
      "X_test:  (37, 10, 21876)\n",
      "y_test: (37, 2, 21876)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import asarray\n",
    "print(\"-----Preprocessing\")\n",
    "# Reading df\n",
    "final = pd.read_csv(\"../data/inputs/tensor_as_df_211801.csv\", sep=\";\", decimal=\",\")\n",
    "# Deleing indexcol\n",
    "final = final.iloc[:, 1:]\n",
    "# Fill na with zeros\n",
    "final = final.fillna(0)\n",
    "\n",
    "print(\"-----Train/test split\")\n",
    "# Param\n",
    "split_time = [209, 209]  # 60%\n",
    "n_features = final.shape[1]\n",
    "# Train test split\n",
    "train_df = final.iloc[:split_time[0], :]\n",
    "test_df = final.iloc[split_time[1]:, :]\n",
    "print(\"Final shape\", final.shape)\n",
    "print(\"Train df shape: \", train_df.shape)\n",
    "print(\"Test df shape: \", test_df.shape)\n",
    "\n",
    "print(\"----Scaling\")\n",
    "\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# transform data\n",
    "scaled_train_df = scaler.fit_transform(train_df)\n",
    "scaled_test_df = scaler.fit_transform(test_df)\n",
    "print(scaled_train_df.shape)\n",
    "print(scaled_test_df.shape)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "def split_sequence(sequence, look_back, forecast_horizon):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        lag_end = i + look_back\n",
    "        forecast_end = lag_end + forecast_horizon\n",
    "        if forecast_end > len(sequence):\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:lag_end], sequence[lag_end:forecast_end]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        X, y = shuffle(X, y, random_state=0)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "print(\"-----Window transformation\")\n",
    "LOOK_BACK = 10\n",
    "FORECAST_RANGE = 2\n",
    "print(\"n-weeks loopback: \", LOOK_BACK)\n",
    "print('n-weeks prediction: ', FORECAST_RANGE)\n",
    "\n",
    "X_train, y_train = split_sequence(\n",
    "    scaled_train_df, look_back=LOOK_BACK, forecast_horizon=FORECAST_RANGE)\n",
    "X_test, y_test = split_sequence(\n",
    "    scaled_test_df, look_back=LOOK_BACK, forecast_horizon=FORECAST_RANGE)\n",
    "\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"Y_train: \", y_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're in a standard tensorflow prediction problem. Here we try to forecast $2$ timesteps in the future given the $10$ past time steps. We have $170k \\times 250$ df, where each columns represents a defined import on $origin \\times product$, and each row a defined week. When no import, data where imputed with 0 values. \n",
    "\n",
    "Parameters : We train on $50$ epochs, batch size of $64$, and validation = 0.1. \n",
    "\n",
    "\n",
    "Differents deep learning paradigms for prediction. Here were in seq2seq or Many2many which means, we give one sequence (of size $10 \\times 170k$) and predict another sequence of size $2 \\times 170k$. Seq2Seq is also used in machine translation when usign a sequence of words to predict another sequence of words.\n",
    "\n",
    "![Seq2Seq](.\\img\\seq2seq.PNG)\n",
    "\n",
    "A typical sequence to sequence model has two parts – an encoder and a decoder. Both the parts are practically two different neural network models combined into one giant network. Broadly, the task of an encoder network is to understand the input sequence, and create a smaller dimensional representation of it. This representation is then forwarded to a decoder network which generates a sequence of its own that represents the output.\n",
    "\n",
    "![enc_dec](.\\img\\enc_dec.PNG)\n",
    "\n",
    "\n",
    "Main networks, are ones wich capturates temporal dependences : \n",
    "\n",
    "- LSTM - ED\n",
    "- GRU - ED\n",
    "- CNN - LSTM - ED\n",
    "- Multi channel\n",
    "\n",
    "\n",
    "Historic : \n",
    "\n",
    "- Decembre 2021 : First models implementation. Loss MAE not OK. Big discrepancies. \n",
    "- Jan 2022 : Changing to MAPE to reduce changes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Callback & internals for tf models\n",
    "import math\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "# Saving model checkpoints and logs\n",
    "checkpoint_filepath = \"../logs\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=False)\n",
    "# Creating callbacks for early stopping if loss do not decreases\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1,\n",
    "    patience=5,\n",
    "    mode='min'\n",
    ")\n",
    "# Reducing learning rate if validation loss stopping improvement\n",
    "rlrop_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, mode='min', patience=3, min_lr=0.00001)\n",
    "\n",
    "# Losses\n",
    "\n",
    "\n",
    "def evaluate_forecast(y_test_inverse, yhat_inverse):\n",
    "    mse_ = tf.keras.losses.MeanSquaredError()\n",
    "    mae_ = tf.keras.losses.MeanAbsoluteError()\n",
    "    mape_ = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "    mae = mae_(y_test_inverse, yhat_inverse)\n",
    "    print('mae:', mae)\n",
    "    mse = mse_(y_test_inverse, yhat_inverse)\n",
    "    print('rmse:', math.sqrt(mse))\n",
    "    mape = mape_(y_test_inverse, yhat_inverse)\n",
    "    print('mape:', mape)\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "validation = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, TimeDistributed, Conv1D, MaxPooling1D, Flatten, Bidirectional, Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - LSTM encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%reload_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "logdir = \"../logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1105WARNING:tensorflow:From C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1133WARNING:tensorflow:From C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.1133 - val_loss: 0.1359\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1299INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.1299 - val_loss: 0.0990\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1063INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.1063 - val_loss: 0.0898\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0973INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0973 - val_loss: 0.0838\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0906INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0906 - val_loss: 0.0808\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0873INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0873 - val_loss: 0.0781\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0848INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0848 - val_loss: 0.0780\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0846INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0846 - val_loss: 0.0766\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0832INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0832 - val_loss: 0.0752\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0818INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0818 - val_loss: 0.0744\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0814INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0814 - val_loss: 0.0741\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0807INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0807 - val_loss: 0.0735\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0800INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0800 - val_loss: 0.0730\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0795INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0795 - val_loss: 0.0725\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0789INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0789 - val_loss: 0.0723\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0787INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0787 - val_loss: 0.0722\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0783INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0783 - val_loss: 0.0719\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0779INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0779 - val_loss: 0.0718\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0776INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0776 - val_loss: 0.0716\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0774INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0774 - val_loss: 0.0714\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0771INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0771 - val_loss: 0.0712\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0767INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0767 - val_loss: 0.0710\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0764INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0764 - val_loss: 0.0709\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0762INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0762 - val_loss: 0.0707\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0758INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0758 - val_loss: 0.0705\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0755INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0755 - val_loss: 0.0704\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0753INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0753 - val_loss: 0.0702\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0751INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0751 - val_loss: 0.0701\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0747INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0747 - val_loss: 0.0701\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0746INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0746 - val_loss: 0.0703\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0746INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 11s 4s/step - loss: 0.0746 - val_loss: 0.0697\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0742INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0742 - val_loss: 0.0697\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0740INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0740 - val_loss: 0.0695\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0737INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0737 - val_loss: 0.0695\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0735INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0735 - val_loss: 0.0693\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0732INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0732 - val_loss: 0.0695\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0731INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0731 - val_loss: 0.0694\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0731INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.0731 - val_loss: 0.0692\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0728INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0728 - val_loss: 0.0691\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0726INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0726 - val_loss: 0.0692\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0725INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0725 - val_loss: 0.0691\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0726INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0726 - val_loss: 0.0688\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0723INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0723 - val_loss: 0.0689\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0721INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0721 - val_loss: 0.0687\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0720INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0720 - val_loss: 0.0687\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0719INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0719 - val_loss: 0.0687\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0718INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0718 - val_loss: 0.0687\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0717INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0717 - val_loss: 0.0687\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0716INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0716 - val_loss: 0.0687\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0716INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0716 - val_loss: 0.0686\n"
     ]
    }
   ],
   "source": [
    "# 1 - Encoder DECODER\n",
    "model_enc_dec = Sequential()\n",
    "# Dim reduction 1 to 100 cells\n",
    "model_enc_dec.add(LSTM(100, activation='relu',\n",
    "                       input_shape=(LOOK_BACK, n_features)\n",
    "                       #kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                       #bias_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "                       ))\n",
    "\n",
    "# Repeat vector X2 (one for each output)\n",
    "model_enc_dec.add(RepeatVector(FORECAST_RANGE))\n",
    "\n",
    "# Prediction with decoder\n",
    "model_enc_dec.add(LSTM(100, activation='relu',\n",
    "                       return_sequences=True\n",
    "                       #kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                       #bias_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "                       ))\n",
    "\n",
    "# Time distributed for many to many pred\n",
    "model_enc_dec.add(TimeDistributed(Dense(n_features, activation = \"relu\")))\n",
    "\n",
    "model_enc_dec.compile(optimizer='adam', loss='mae')\n",
    "plot_model(model=model_enc_dec, show_shapes=True,\n",
    "           to_file=\"./img/model_enc_dec.png\")\n",
    "\n",
    "history = model_enc_dec.fit(X_train, y_train,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_split=validation,\n",
    "                            callbacks=[\n",
    "                            #early_stopping_callback,\n",
    "                            checkpoint_callback,\n",
    "                            rlrop_callback,\n",
    "                            tensorboard_callback])\n",
    "\n",
    "yhat = model_enc_dec.predict(X_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17340), started 6 days, 4:17:26 ago. (Use '!kill 17340' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bf5438b5c32700df\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bf5438b5c32700df\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ../logs/scalars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yhat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18236/2253764220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mget_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'yhat' is not defined"
     ]
    }
   ],
   "source": [
    "# Metrics on first timestep\n",
    "# Extract first timesteps from y-test and y-hat\n",
    "def get_metric(y_hat_parm):\n",
    "    first_test = y_test[:, 0, :]\n",
    "    first_pred = y_hat_parm[:, 0, :]\n",
    "    # Apply inverse transform to y-hat extracted arrays\n",
    "    first_pred = scaler.inverse_transform(first_pred)\n",
    "    # Calculating metrics\n",
    "    print(\"Metrics, TimeStep One -------------\")\n",
    "    print(evaluate_forecast(first_test, first_pred))\n",
    "\n",
    "    # Metrics on second timestep\n",
    "    print(\"Metrics, TimeStep Two -------------\")\n",
    "    second_test = y_test[:, 1, :]\n",
    "    second_pred = y_hat_parm[:, 1, :]\n",
    "    second_pred = scaler.inverse_transform(second_pred)\n",
    "    print(evaluate_forecast(second_test, second_pred))\n",
    "    return(\"DONE\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - BIS GRU-ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1091INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.1091 - val_loss: 0.0912\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0989INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.0989 - val_loss: 0.0839\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0919INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0919 - val_loss: 0.0819\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0899INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0899 - val_loss: 0.0806\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0884INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0884 - val_loss: 0.0797\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0872INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 9s 3s/step - loss: 0.0872 - val_loss: 0.0788\n"
     ]
    }
   ],
   "source": [
    "# 1 - Encoder DECODER\n",
    "gru_model_enc_dec = Sequential()\n",
    "# Dim reduction 1 to 100 cells\n",
    "gru_model_enc_dec.add(GRU(100, activation='relu',\n",
    "                          input_shape=(LOOK_BACK, n_features)\n",
    "                          #kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                          #bias_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "                          ))\n",
    "\n",
    "# Repeat vector X2 (one for each output)\n",
    "gru_model_enc_dec.add(RepeatVector(FORECAST_RANGE))\n",
    "\n",
    "# Prediction with decoder\n",
    "gru_model_enc_dec.add(GRU(100, activation='relu',\n",
    "                          return_sequences=True\n",
    "                          #kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                          #bias_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "                          ))\n",
    "\n",
    "# Time distributed for many to many pred\n",
    "gru_model_enc_dec.add(TimeDistributed(Dense(n_features, activation = \"relu\")))\n",
    "\n",
    "gru_model_enc_dec.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "\n",
    "plot_model(model=gru_model_enc_dec, show_shapes=True,\n",
    "           to_file=\"./img/model_enc_dec_gru.png\")\n",
    "history = gru_model_enc_dec.fit(X_train, y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_split=validation,\n",
    "                                callbacks=[early_stopping_callback, checkpoint_callback, rlrop_callback])\n",
    "\n",
    "yhat_gru = gru_model_enc_dec.predict(X_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics, TimeStep One -------------\n",
      "mae: tf.Tensor(74382.31, shape=(), dtype=float32)\n",
      "rmse: 1018862.5686951111\n",
      "mape: tf.Tensor(6182428000000.0, shape=(), dtype=float32)\n",
      "None\n",
      "Metrics, TimeStep Two -------------\n",
      "mae: tf.Tensor(80622.22, shape=(), dtype=float32)\n",
      "rmse: 1092641.7089458008\n",
      "mape: tf.Tensor(8420258000000.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric(yhat_gru)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B - CNN - LSTM encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1096INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.1096 - val_loss: 0.0942\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1024INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.1024 - val_loss: 0.0865\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0942INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0942 - val_loss: 0.0815\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0892INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0892 - val_loss: 0.0791\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0870INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0870 - val_loss: 0.0789\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0866INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 7s 2s/step - loss: 0.0866 - val_loss: 0.0778\n"
     ]
    }
   ],
   "source": [
    "model_enc_dec_cnn = Sequential()\n",
    "# 1 - Encoder x 2 Conv 1D + Max pooling\n",
    "model_enc_dec_cnn.add(Conv1D(filters=64, kernel_size=4,\n",
    "                      activation='relu', input_shape=(LOOK_BACK, n_features)))\n",
    "model_enc_dec_cnn.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
    "model_enc_dec_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_enc_dec_cnn.add(Flatten())\n",
    "# Reapet vector\n",
    "model_enc_dec_cnn.add(RepeatVector(FORECAST_RANGE))\n",
    "\n",
    "# 2 - Decoder : LSTM 200\n",
    "model_enc_dec_cnn.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "# Time distrubuted 1\n",
    "model_enc_dec_cnn.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model_enc_dec_cnn.add(TimeDistributed(Dense(n_features, activation = \"relu\")))\n",
    "model_enc_dec_cnn.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "plot_model(model=model_enc_dec_cnn, show_shapes=True,\n",
    "           to_file=\"./img/model_enc_dec_cnn.png\")\n",
    "history = model_enc_dec_cnn.fit(X_train, y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_split=validation,\n",
    "                                callbacks=[early_stopping_callback, checkpoint_callback, rlrop_callback])\n",
    "\n",
    "yhat_cnn = model_enc_dec_cnn.predict(X_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics, TimeStep One -------------\n",
      "mae: tf.Tensor(87681.92, shape=(), dtype=float32)\n",
      "rmse: 1274556.154852347\n",
      "mape: tf.Tensor(8054249000000.0, shape=(), dtype=float32)\n",
      "None\n",
      "Metrics, TimeStep Two -------------\n",
      "mae: tf.Tensor(88592.83, shape=(), dtype=float32)\n",
      "rmse: 1281348.9723693542\n",
      "mape: tf.Tensor(10152520000000.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metric(yhat_cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C - Multi channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0467WARNING:tensorflow:From C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0473 - val_loss: 0.0417\n",
      "Epoch 2/50\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0472INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0468 - val_loss: 0.0408\n",
      "Epoch 3/50\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0455INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0454 - val_loss: 0.0372\n",
      "Epoch 4/50\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0400INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0403 - val_loss: 0.0286\n",
      "Epoch 5/50\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0336INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0331 - val_loss: 0.0268\n",
      "Epoch 6/50\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0309INFO:tensorflow:Assets written to: ..\\logs\\assets\n",
      "3/3 [==============================] - 8s 3s/step - loss: 0.0307 - val_loss: 0.0255\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(LOOK_BACK, n_features))\n",
    "conv = Conv1D(filters=4, kernel_size=4, activation='relu')(input_layer)\n",
    "conv = Conv1D(filters=6, kernel_size=7, activation='relu')(conv)\n",
    "lstm = LSTM(100, return_sequences=True, activation='relu')(conv)\n",
    "dropout = Dropout(0.2)(lstm)\n",
    "lstm = LSTM(100, activation='relu')(dropout)\n",
    "dense = Dense(FORECAST_RANGE*n_features, activation='relu')(lstm)\n",
    "output_layer = Reshape((FORECAST_RANGE, n_features))(dense)\n",
    "model_vector_output = Model([input_layer], [output_layer])\n",
    "model_vector_output.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "plot_model(model=model_vector_output, show_shapes=True,\n",
    "           to_file=\"./img/model_vector_output.png\")\n",
    "history = model_vector_output.fit(X_train, y_train,\n",
    "                                  epochs=epochs,\n",
    "                                  batch_size=batch_size,\n",
    "                                  validation_split=validation,\n",
    "                                  callbacks=[early_stopping_callback, checkpoint_callback, rlrop_callback])\n",
    "\n",
    "yhat_multi_channel = model_vector_output.predict(X_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model 2\n",
    "model_vector_output.save(\"../models/vector_output.h5\")\n",
    "# Saving model 1\n",
    "model_enc_dec.save(\"../models/gru_model_enc_dec.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Analysis and diagnosis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an anomaly analysis where the difference between predicted and realised values are strong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual matrix shape:  (37, 21876)\n",
      "residual per variable:  (21876,)\n",
      "residual temporels:  (37,)\n"
     ]
    }
   ],
   "source": [
    "# Inverse scaling for getting truevals\n",
    "first_pred = scaler.inverse_transform(yhat_multi_channel[:, 0, :])\n",
    "first_train = scaler.inverse_transform(y_train[:, 0, :])\n",
    "first_test = scaler.inverse_transform(y_test[:, 0, :])\n",
    "\n",
    "# Creating a residual matrix for one timestep prediction\n",
    "residual_matrix = abs(first_test-first_pred)\n",
    "print(\"residual matrix shape: \", residual_matrix.shape)\n",
    "# Residus per col and rows\n",
    "residus_per_variable = np.sum(\n",
    "    residual_matrix, axis=0) / residual_matrix.shape[0]\n",
    "residus_temporels = np.sum(residual_matrix, axis=1) / residual_matrix.shape[1]\n",
    "\n",
    "# Shape verifs\n",
    "print(\"residual per variable: \", residus_per_variable.shape)\n",
    "print(\"residual temporels: \", residus_temporels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         57966.486486\n",
       "1         10598.703191\n",
       "2        101228.147804\n",
       "3          4058.837838\n",
       "4           716.569356\n",
       "             ...      \n",
       "21871     18116.067541\n",
       "21872    393488.525496\n",
       "21873      1764.135135\n",
       "21874         0.103319\n",
       "21875     23216.501689\n",
       "Name: residuals, Length: 21876, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residus_per_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DGDDI\\AppData\\Local\\Temp/ipykernel_18236/893959555.py:3: RuntimeWarning: divide by zero encountered in log10\n",
      "  logbins = np.logspace(np.log10(bins[0]), np.log10(bins[-1]), len(bins))\n",
      "C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\numpy\\core\\function_base.py:153: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\numpy\\core\\function_base.py:163: RuntimeWarning: invalid value encountered in add\n",
      "  y += start\n",
      "C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:433: RuntimeWarning: invalid value encountered in greater\n",
      "  if np.any(bin_edges[:-1] > bin_edges[1:]):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = residus_per_variable_df[\"residuals\"]\n",
    "hist, bins, _ = plt.hist(x, bins=10000)\n",
    "logbins = np.logspace(np.log10(bins[0]), np.log10(bins[-1]), len(bins))\n",
    "plt.hist(x, bins=logbins)\n",
    "# plt.xscale('log')\n",
    "plt.xlim(left = -0.5, right=1000000)\n",
    "plt.title(\"Repartition des residus moyen par couple NC x Produit\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          residuals\n",
      "count  2.187600e+04\n",
      "mean   1.779678e+05\n",
      "std    2.416045e+06\n",
      "min    0.000000e+00\n",
      "50%    2.210043e+04\n",
      "75%    6.569195e+04\n",
      "80%    8.765297e+04\n",
      "90%    1.918256e+05\n",
      "99%    1.812741e+06\n",
      "99.9%  2.159798e+07\n",
      "max    1.800326e+08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DGDDI\\AppData\\Local\\Temp/ipykernel_18236/3860244622.py:13: RuntimeWarning: divide by zero encountered in log10\n",
      "  logbins = np.logspace(np.log10(bins[0]), np.log10(bins[-1]), len(bins))\n",
      "C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\numpy\\core\\function_base.py:153: RuntimeWarning: invalid value encountered in multiply\n",
      "  y *= step\n",
      "C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\numpy\\core\\function_base.py:163: RuntimeWarning: invalid value encountered in add\n",
      "  y += start\n",
      "C:\\Users\\DGDDI\\anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py:433: RuntimeWarning: invalid value encountered in greater\n",
      "  if np.any(bin_edges[:-1] > bin_edges[1:]):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               columns     residuals\n",
      "11214                   Inde8708803590  32998.112595\n",
      "15947  Russie, Fédération de8803300099  32988.574166\n",
      "15563            Royaume-Uni8442500000  32985.783784\n",
      "18433                Tunisie6107110000  32977.082981\n",
      "2814                   Chine7317006090  32972.316749\n",
      "13741                Maurice9603301000  32971.543180\n",
      "13506                  Maroc7419999099  32960.837416\n",
      "7988                  France0406909300  32960.798142\n",
      "13281                  Maroc3301909000  32957.918736\n",
      "16236              Singapour8548909099  32955.986856\n"
     ]
    }
   ],
   "source": [
    "# 1 - Residus per SH analysis\n",
    "# Adding labels\n",
    "import matplotlib.pyplot as plt\n",
    "cols = pd.Series(final.columns, name='columns')\n",
    "residus_per_variable = pd.Series(residus_per_variable, name='residuals')\n",
    "residus_per_variable_df = pd.concat([cols, residus_per_variable], axis=1)\n",
    "\n",
    "print(residus_per_variable_df.describe(percentiles = [0.5,0.75,0.8,0.9,0.99,0.999]))\n",
    "# Stats descs\n",
    "# Distribution of residuals per countries x NC (in €)\n",
    "x = residus_per_variable_df[\"residuals\"]\n",
    "hist, bins, _ = plt.hist(x, bins=100)\n",
    "logbins = np.logspace(np.log10(bins[0]), np.log10(bins[-1]), len(bins))\n",
    "plt.subplot(212)\n",
    "plt.hist(x, bins=logbins)\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "# TOP 0.001 % of NC (outliers ?)\n",
    "quantile = residus_per_variable_df.residuals.quantile(0.6)\n",
    "print(residus_per_variable_df[residus_per_variable_df.residuals <\n",
    "                        quantile].sort_values(\"residuals\", ascending=False)[0:10])\n",
    "\n",
    "col_list = residus_per_variable_df[residus_per_variable_df.residuals <\n",
    "                        quantile].sort_values(\"residuals\", ascending=False)[0:10][\"columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis  specific serie\n",
    "\n",
    "def serie_analysis(index_col):\n",
    "\n",
    "    selected_index = cols[cols == index_col].index[0]\n",
    "    \n",
    "    print(\"train_serie\")\n",
    "    train_serie = list(first_train[:, selected_index])\n",
    "    print(train_serie)\n",
    "        \n",
    "    print(\"residuals\")\n",
    "    residuals_serie = list(residual_matrix[:, selected_index])\n",
    "    print(residuals_serie)\n",
    "\n",
    "    print(\"pred_serie\")\n",
    "    pred_serie = list(first_pred[:, selected_index])\n",
    "    print(pred_serie)\n",
    "\n",
    "    print(\"test_serie\")\n",
    "    test_serie = list(first_test[:, selected_index])\n",
    "    print(test_serie)\n",
    "\n",
    "    # TODO - PLOT NORMAL VS RESIDUALS\n",
    "\n",
    "\n",
    "    # plot each series\n",
    "    plt.title(index_col)\n",
    "    #fig, ax = plt.subplots()\n",
    "    #ax.axis([-1, 10000, 1, 10000000])\n",
    "    #ax.loglog()\n",
    "    plt.plot(residuals_serie, label=\"predicted\")\n",
    "    plt.plot(test_serie, label=\"true values\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.yscale(\"log\")\n",
    "    #plt.legend(handles=[pred_serie, pred_serie])\n",
    "    plt.show()\n",
    "    return(\"DONE\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b56d35f92f1121b08366478e1e5a05487b5a6bcb839bbf65f00731116ee909e3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
